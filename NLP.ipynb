{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2234f0d",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c26a9af0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Natural', 'language', 'Processing', 'is', 'a', 'Fascinating', 'field', 'of', 'study', '.']\n",
      "Lemmas: ['natural', 'language', 'processing', 'be', 'a', 'fascinating', 'field', 'of', 'study', '.']\n",
      "Natural amod language NOUN []\n",
      "language compound Processing NOUN [Natural]\n",
      "Processing nsubj is AUX [language]\n",
      "is ROOT is AUX [Processing, field, .]\n",
      "a det field NOUN []\n",
      "Fascinating amod field NOUN []\n",
      "field attr is AUX [a, Fascinating, of]\n",
      "of prep field NOUN [study]\n",
      "study pobj of ADP []\n",
      ". punct is AUX []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "text=\"Natural language Processing is a Fascinating field of study.\"\n",
    "\n",
    "doc=nlp(text)\n",
    "tokens=[token.text for token in doc]\n",
    "lemmas=[token.lemma_ for token in doc]\n",
    "\n",
    "print(\"Tokens:\",tokens)\n",
    "print(\"Lemmas:\",lemmas)\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,token.dep_,token.head.text,token.head.pos_,[child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8da7bc",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b95e5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Feedback 1: 'The product is amazing! I love the quality.'\n",
      "Tokens: ['The', 'product', 'is', 'amazing', '!', 'I', 'love', 'the', 'quality', '.']\n",
      "Lemmas: ['the', 'product', 'be', 'amazing', '!', 'I', 'love', 'the', 'quality', '.']\n",
      "\n",
      "Dependency Parsing:\n",
      "The det product NOUN []\n",
      "product nsubj is AUX [The]\n",
      "is ROOT is AUX [product, amazing, !]\n",
      "amazing acomp is AUX []\n",
      "! punct is AUX []\n",
      "I nsubj love VERB []\n",
      "love ROOT love VERB [I, quality, .]\n",
      "the det quality NOUN []\n",
      "quality dobj love VERB [the]\n",
      ". punct love VERB []\n",
      "\n",
      "Analyzing Feedback 2: 'The customer service was terrible, very disappointed.'\n",
      "Tokens: ['The', 'customer', 'service', 'was', 'terrible', ',', 'very', 'disappointed', '.']\n",
      "Lemmas: ['the', 'customer', 'service', 'be', 'terrible', ',', 'very', 'disappointed', '.']\n",
      "\n",
      "Dependency Parsing:\n",
      "The det service NOUN []\n",
      "customer compound service NOUN []\n",
      "service nsubj was AUX [The, customer]\n",
      "was ROOT was AUX [service, disappointed, .]\n",
      "terrible amod disappointed ADJ []\n",
      ", punct disappointed ADJ []\n",
      "very advmod disappointed ADJ []\n",
      "disappointed acomp was AUX [terrible, ,, very]\n",
      ". punct was AUX []\n",
      "\n",
      "Analyzing Feedback 3: 'Great experience overall, highly recommended.'\n",
      "Tokens: ['Great', 'experience', 'overall', ',', 'highly', 'recommended', '.']\n",
      "Lemmas: ['great', 'experience', 'overall', ',', 'highly', 'recommend', '.']\n",
      "\n",
      "Dependency Parsing:\n",
      "Great amod experience NOUN []\n",
      "experience nsubj recommended VERB [Great]\n",
      "overall advmod recommended VERB []\n",
      ", punct recommended VERB []\n",
      "highly advmod recommended VERB []\n",
      "recommended ROOT recommended VERB [experience, overall, ,, highly, .]\n",
      ". punct recommended VERB []\n",
      "\n",
      "Analyzing Feedback 4: 'The delivery was late, very frustrating.'\n",
      "Tokens: ['The', 'delivery', 'was', 'late', ',', 'very', 'frustrating', '.']\n",
      "Lemmas: ['the', 'delivery', 'be', 'late', ',', 'very', 'frustrating', '.']\n",
      "\n",
      "Dependency Parsing:\n",
      "The det delivery NOUN []\n",
      "delivery nsubj was AUX [The]\n",
      "was ROOT was AUX [delivery, frustrating, .]\n",
      "late advmod frustrating ADJ []\n",
      ", punct frustrating ADJ []\n",
      "very advmod frustrating ADJ []\n",
      "frustrating acomp was AUX [late, ,, very]\n",
      ". punct was AUX []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "customer_feedback = [\n",
    " \"The product is amazing! I love the quality.\",\n",
    " \"The customer service was terrible, very disappointed.\",\n",
    " \"Great experience overall, highly recommended.\",\n",
    " \"The delivery was late, very frustrating.\"\n",
    "]\n",
    "def analyze_feedback(feedback):\n",
    "     for idx, text in enumerate(feedback, start=1):\n",
    "            print(f\"\\nAnalyzing Feedback {idx}: '{text}'\")\n",
    "            doc = nlp(text)\n",
    "            # Extract tokens and lemmatization\n",
    "            tokens = [token.text for token in doc]\n",
    "            lemmas = [token.lemma_ for token in doc]\n",
    "            print(\"Tokens:\", tokens)\n",
    "            print(\"Lemmas:\", lemmas)\n",
    "            # Dependency parsing\n",
    "            print(\"\\nDependency Parsing:\")\n",
    "            for token in doc:\n",
    "                print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "                [child for child in token.children])\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "     analyze_feedback(customer_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935c300",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b09fb88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\online\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\online\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the works of Chozeba , helpe , Hear a wounded him and have transgressed , in dirty lane came forth and\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "words = nltk.corpus.gutenberg.words()\n",
    "\n",
    "bigrams = list(nltk.bigrams(words))\n",
    "\n",
    "starting_word = \"the\"\n",
    "generated_text = [starting_word]\n",
    "\n",
    "for _ in range(20):\n",
    "    possible_words = [word2 for (word1, word2) in bigrams if word1.lower() ==\n",
    "generated_text[-1].lower()]\n",
    "\n",
    "\n",
    "    next_word = random.choice(possible_words)\n",
    "    generated_text.append(next_word)\n",
    "\n",
    "print(' '.join(generated_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e728393e",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60feb323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.37.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.3.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.12.7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1df884905c8471bbe1cc02084f171e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\online\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\online\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfe60b3cd004ce08e3ccd33369ac5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c9de91177843239850c6ebc7203a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a1b53a11614ef19e6b61a86313aa73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d895827b1044cfa4db7ce317b934db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf19a9169066413fbc5ea7347a5bd883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b586a1934e4628860460b7cffa162b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your sentence (type 'exit' to end): Hi this is Gopi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autocomplete Suggestions: ['Hi', '[Recipient],', 'Hi', 'this', 'is', 'Gopi.', 'I', 'am', 'a', 'member', 'of', 'the', 'Project', 'Project', 'team', 'and', 'I', 'have', 'been', 'working', 'on', 'a', 'project', 'for', 'a', 'while', 'now.', 'The', 'project', 'is', 'called', 'Project', 'PROJECT.']\n",
      "Enter your sentence (type 'exit' to end): exit\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "class EmailAutocompleteSystem:\n",
    "    def __init__(self):\n",
    "        self.model_name = \"gpt2\"\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(self.model_name)\n",
    "    def generate_suggestions(self, user_input, context):\n",
    "        input_text = f\"{context} {user_input}\"\n",
    "        input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "            generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            suggestions = generated_text.split()[len(user_input.split()):]\n",
    "        return suggestions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    autocomplete_system = EmailAutocompleteSystem()\n",
    "\n",
    "    email_context = \"Subject: Discussing Project Proposal\\nHi [Recipient],\"\n",
    "while True:\n",
    "    user_input = input(\"Enter your sentence (type 'exit' to end): \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    suggestions = autocomplete_system.generate_suggestions(user_input, email_context)\n",
    "    if suggestions:\n",
    "        print(\"Autocomplete Suggestions:\", suggestions)\n",
    "    else:\n",
    "        print(\"No suggestions available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425de232",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c893b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.23.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.9.2)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "categories = ['sci.med', 'sci.space', 'comp.graphics', 'talk.politics.mideast']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "X_train = newsgroups_train.data\n",
    "X_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "model = make_pipeline(\n",
    " TfidfVectorizer(),\n",
    " LinearSVC()\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e686ef",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4f4344d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9389623601220752\n",
      "\n",
      "Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "comp.sys.ibm.pc.hardware       0.92      0.91      0.91       212\n",
      "   comp.sys.mac.hardware       0.94      0.93      0.94       198\n",
      "               rec.autos       0.97      0.93      0.95       179\n",
      "         rec.motorcycles       0.96      0.99      0.97       205\n",
      "         sci.electronics       0.92      0.93      0.92       189\n",
      "\n",
      "                accuracy                           0.94       983\n",
      "               macro avg       0.94      0.94      0.94       983\n",
      "            weighted avg       0.94      0.94      0.94       983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=['comp.sys.ibm.pc.hardware',\n",
    "'comp.sys.mac.hardware', 'rec.autos', 'rec.motorcycles', 'sci.electronics'])\n",
    "\n",
    "X = newsgroups.data\n",
    "y = newsgroups.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions, target_names=newsgroups.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba80804",
   "metadata": {},
   "source": [
    "# 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbba4a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.23.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.9.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\online\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.3.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install nltk\n",
    "import gensim.downloader as api\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_vectors = api.load(\"word2vec-google-news-300\")\n",
    "sentences = [\n",
    "\"Natural language processing is a challenging but fascinating field.\",\n",
    "\"Word embeddings capture semantic meanings of words in a vector space.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "for tokenized_sentence in tokenized_sentences:\n",
    "    for word in tokenized_sentence:\n",
    "        if word in word_vectors:\n",
    "            similar_words = word_vectors.most_similar(word)\n",
    "            print(f\"Words similar to '{word}': {similar_words}\")\n",
    "        else:\n",
    "            print(f\"'{word}' is not in the pre-trained Word2Vec model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2608b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
